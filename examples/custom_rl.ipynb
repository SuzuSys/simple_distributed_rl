{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = %pwd\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.join(pwd, \"../\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "-------\n",
    "\n",
    "自作の強化学習アルゴリズムを作成するには以下のクラスを定義します。  \n",
    "  \n",
    "RLConfig  \n",
    "RLRemoteMemory  \n",
    "RLParameter  \n",
    "RLTrainer  \n",
    "RLWorker  \n",
    "\n",
    "ここでは一番簡単なテーブル型のQ学習を自作してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Tuple, Dict, Union, cast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLConfig\n",
    "\n",
    "---\n",
    "強化学習アルゴリズムの種類やハイパーパラメータ等を管理するクラスです。  \n",
    "RLConfigを継承して作成してもいいですが、強化学習の種類によるインタフェースも提供しています。  \n",
    "'srl.rl.algorithms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from srl.base.rl.algorithms.table import TableConfig\n",
    "\n",
    "@dataclass\n",
    "class MyConfig(TableConfig):\n",
    "    \n",
    "    # ハイパーパラメータ\n",
    "    epsilon: float = 0.1\n",
    "    test_epsilon: float = 0\n",
    "    gamma: float = 0.9\n",
    "    lr: float = 0.1\n",
    "\n",
    "    # 名前だけユニークなものを指定\n",
    "    @staticmethod\n",
    "    def getName() -> str:\n",
    "        return \"MyRL\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RemoteMemory\n",
    "\n",
    "---\n",
    "Workerが取得した情報(経験)をTrainerに渡す役割を持っているクラスです。  \n",
    "分散学習では multiprocessing のサーバプロセス(Manager)になります。  \n",
    "ですので、変数へのアクセスができなくなる点だけ制約があります。（全て関数経由でやり取りする必要があります）  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srl.base.rl.base import RLRemoteMemory\n",
    "\n",
    "class MyRemoteMemory(RLRemoteMemory):\n",
    "\n",
    "    # init の引数は親クラスにそのまま渡します。\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        # self.config は上で定義した MyConfig が入っています\n",
    "        self.config = cast(MyConfig, self.config)\n",
    "\n",
    "        # 経験を一時的に保存する用\n",
    "        self.buffer = []\n",
    "\n",
    "    # (abstractmethod) メモリに保存されている数を返す\n",
    "    def length(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    # (abstractmethod) restore/backupで復元できるように作成\n",
    "    def restore(self, data: Any) -> None:\n",
    "        self.buffer = data\n",
    "\n",
    "    def backup(self):\n",
    "        return self.buffer\n",
    "\n",
    "    # --- 以下は独自に定義している関数です\n",
    "\n",
    "    def add(self, batch: Any) -> None:\n",
    "        self.buffer.append(batch)\n",
    "\n",
    "    def sample(self):\n",
    "        buffer = self.buffer\n",
    "        self.buffer = []\n",
    "        return buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "またよくあるクラスは 'srl.rl.remote_memoty' に定義しています。  \n",
    "経験を順番通りに取り出す SequenceRemoteMemory を使う場合は以下です。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srl.rl.remote_memory.sequence_memory import SequenceRemoteMemory\n",
    "\n",
    "class MyRemoteMemory(SequenceRemoteMemory):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLParameter\n",
    "\n",
    "---\n",
    "学習する/したパラメータを管理するクラスです。  \n",
    "ある意味強化学習の本体でもあります。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from srl.base.rl.base import RLParameter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MyParameter(RLParameter):\n",
    "\n",
    "    # init の引数は親クラスにそのまま渡します。\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        # self.config は上で定義した MyConfig が入っています\n",
    "        self.config = cast(MyConfig, self.config)\n",
    "\n",
    "        # Q学習用のテーブル\n",
    "        self.Q = {}\n",
    "\n",
    "    # (abstractmethod) restore/backupでQテーブルを復元できるように作成\n",
    "    def restore(self, data: Any) -> None:\n",
    "        self.Q = json.loads(data)\n",
    "\n",
    "    def backup(self):\n",
    "        return json.dumps(self.Q)\n",
    "\n",
    "    # Q値を取得する関数\n",
    "    def get_action_values(self, state: str, invalid_actions):\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = [-np.inf if a in invalid_actions else 0 for a in range(self.config.nb_actions)]\n",
    "        return self.Q[state]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLWorker\n",
    "\n",
    "---\n",
    "経験を収集するクラスです。  \n",
    "実際に環境と連携して動作する部分にもなります。  \n",
    "役割は、Parameterを参照してアクションを決める事と、経験をMemoryに送信する事です。  \n",
    "\n",
    "RLWorkerをそのまま継承しても問題ありませんが、  \n",
    "作成するアルゴリズムによりインタフェースが変わるので、  \n",
    "アルゴリズムに合わせたクラスを継承しても問題ありません。  \n",
    "\n",
    "また、RLWorkerはエピソード内で同じインスタンスが使いまわされます。  \n",
    "フローとしては以下です。  \n",
    "\n",
    "``` python\n",
    "env.reset()\n",
    "worker.on_reset()\n",
    "while:\n",
    "    worker.policy()\n",
    "    env.step()\n",
    "    worker.on_step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from srl.base.rl.algorithms.table import TableWorker\n",
    "from srl.base.env.env_for_rl import EnvForRL\n",
    "\n",
    "class MyWorker(TableWorker):\n",
    "\n",
    "    # init の引数は親クラスにそのまま渡します。\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "\n",
    "        # 上で定義した config,parameter,remote_memory が入ります\n",
    "        self.config = cast(MyConfig, self.config)\n",
    "        self.parameter = cast(MyParameter, self.parameter)\n",
    "        self.remote_memory = cast(MyRemoteMemory, self.remote_memory)\n",
    "\n",
    "    # エピソードの最初に呼ばれる関数\n",
    "    # state: 環境の初期状態が入っています\n",
    "    # invalid_actions: 有効でないアクションのリストです\n",
    "    def call_on_reset(self, state: np.ndarray, invalid_actions: List[int]) -> None:\n",
    "        # 初期状態を保存\n",
    "        self.state = str(state.tolist())\n",
    "        self.invalid_actions = invalid_actions\n",
    "\n",
    "        if self.training:\n",
    "            self.epsilon = self.config.epsilon\n",
    "        else:\n",
    "            self.epsilon = self.config.test_epsilon\n",
    "\n",
    "    # 戦略部分です。アクションを返します。\n",
    "    # state: 環境の状態が入っています\n",
    "    # invalid_actions: 有効でないアクションのリストです\n",
    "    # 戻り値はアクションです。\n",
    "    def call_policy(self, state: np.ndarray, invalid_actions: List[int]) -> int:\n",
    "        # アクション前の状態を保存\n",
    "        self.state = str(state.tolist())\n",
    "        self.invalid_actions = invalid_actions\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            # epsilonより低いならランダムに移動\n",
    "            action = random.choice([a for a in range(self.config.nb_actions) if a not in invalid_actions])\n",
    "        else:\n",
    "            q = self.parameter.get_action_values(self.state, invalid_actions)\n",
    "            q = np.asarray(q)\n",
    "\n",
    "            # 最大値を選ぶ（複数あればランダム）\n",
    "            action = random.choice(np.where(q == q.max())[0])\n",
    "\n",
    "        self.action = int(action)\n",
    "        return self.action\n",
    "\n",
    "    # 1step毎に呼ばれる関数です\n",
    "    # next_state: アクション実行後の状態\n",
    "    # reweard   : アクション実行後の報酬\n",
    "    # done      : アクション実行後の終了状態\n",
    "    # next_invalid_actions: アクション実行後の有効でないアクションのリスト\n",
    "    def call_on_step(\n",
    "        self,\n",
    "        next_state: Any,\n",
    "        reward: float,\n",
    "        done: bool,\n",
    "        next_invalid_actions: List[int],\n",
    "    ) -> Dict[str, Union[float, int]]:\n",
    "        if not self.training:\n",
    "            return {}\n",
    "        \n",
    "        batch = {\n",
    "            \"state\": self.state,\n",
    "            \"next_state\": str(next_state.tolist()),\n",
    "            \"action\": self.action,\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "            \"invalid_actions\": self.invalid_actions,\n",
    "            \"next_invalid_actions\": next_invalid_actions,\n",
    "        }\n",
    "        self.remote_memory.add(batch)\n",
    "        return {}\n",
    "\n",
    "    # 強化学習の可視化用\n",
    "    # 今回ですと、Qテーブルを表示しています。\n",
    "    def render(self, env: EnvForRL) -> None:\n",
    "        q = self.parameter.get_action_values(self.state, self.invalid_actions)\n",
    "        maxa = np.argmax(q)\n",
    "        for a in range(self.config.nb_actions):\n",
    "            if a == maxa:\n",
    "                s = \"*\"\n",
    "            else:\n",
    "                s = \" \"\n",
    "            s += f\"{env.action_to_str(a)}: {q[a]:7.5f}\"\n",
    "            print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLTrainer\n",
    "\n",
    "---\n",
    "学習を定義する部分です。  \n",
    "RemoteMemory から経験を受け取ってParameterを更新します。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srl.base.rl.base import RLTrainer\n",
    "\n",
    "class MyTrainer(RLTrainer):\n",
    "\n",
    "    # init の引数は親クラスにそのまま渡します。\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "\n",
    "        # 上で定義した config,parameter,memory が入ります\n",
    "        self.config = cast(MyConfig, self.config)\n",
    "        self.parameter = cast(MyParameter, self.parameter)\n",
    "        self.remote_memory = cast(MyRemoteMemory, self.remote_memory)\n",
    "\n",
    "        self.train_count = 0\n",
    "\n",
    "    # (abstractmethod) 学習回数を返します\n",
    "    def get_train_count(self):\n",
    "        return self.train_count\n",
    "\n",
    "    # (abstractmethod)\n",
    "    def train(self):\n",
    "\n",
    "        # memoryから経験を取得する\n",
    "        batchs = self.remote_memory.sample()\n",
    "        td_error = 0\n",
    "        for batch in batchs:\n",
    "            # 各batch毎にQテーブルを更新する\n",
    "\n",
    "            s = batch[\"state\"]\n",
    "            n_s = batch[\"next_state\"]\n",
    "            action = batch[\"action\"]\n",
    "            reward = batch[\"reward\"]\n",
    "            done = batch[\"done\"]\n",
    "            invalid_actions = batch[\"invalid_actions\"]\n",
    "            next_invalid_actions = batch[\"next_invalid_actions\"]\n",
    "\n",
    "            q = self.parameter.get_action_values(s, invalid_actions)\n",
    "            n_q = self.parameter.get_action_values(n_s, next_invalid_actions)\n",
    "\n",
    "            if done:\n",
    "                target_q = reward\n",
    "            else:\n",
    "                target_q = reward + self.config.gamma * max(n_q)\n",
    "\n",
    "            td_error = target_q - q[action]\n",
    "            q[action] += self.config.lr * td_error\n",
    "\n",
    "            td_error += td_error\n",
    "            self.train_count += 1\n",
    "\n",
    "        if len(batchs) > 0:\n",
    "            td_error /= len(batchs)\n",
    "        \n",
    "        # 学習結果の情報を返す\n",
    "        return {\n",
    "            \"Q\": len(self.parameter.Q),\n",
    "            \"td_error\": td_error,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 登録"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後にこれらのクラスを登録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srl.base.rl.registration import register\n",
    "register(\n",
    "    MyConfig,\n",
    "    __name__ + \":MyRemoteMemory\",\n",
    "    __name__ + \":MyParameter\",\n",
    "    __name__ + \":MyTrainer\",\n",
    "    __name__ + \":MyWorker\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### env: FrozenLake-v1, max episodes: -1, max steps: 10, timeout:  -1.00s\n",
      "17:53:36   0.00s       2ep      11tr  -0.00s(remain), 0.000 0.000 0.000 reward, 4.0 step, 0.00s/ep, 0.0000s/tr,        0 mem|Q 3.500|td_error 0.000\n",
      "### 0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      " 2: 0.00000\n",
      " 3: 0.00000\n",
      "### 1, done: False\n",
      "player 0, action 2, reward: 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "env_info  : {'prob': 0.3333333333333333}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      " 2: 0.00000\n",
      " 3: 0.00000\n",
      "### 2, done: True\n",
      "player 0, action 3, reward: 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "env_info  : {'prob': 0.3333333333333333}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "### env: OneRoad, max episodes: -1, max steps: 10, timeout:  -1.00s\n",
      "17:53:36   0.00s       0ep      11tr  -0.00s(remain), 0.000 0.000 0.000 reward, 11.0 step, 0.00s/ep, 0.0000s/tr,        0 mem|Q 5.636|td_error 0.000\n",
      "### 0\n",
      "0 / 10\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      "### 1, done: False\n",
      "player 0, action 0, reward: 0.0\n",
      "1 / 10\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      "### 2, done: False\n",
      "player 0, action 0, reward: 0.0\n",
      "2 / 10\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      "### 3, done: False\n",
      "player 0, action 0, reward: 0.0\n",
      "3 / 10\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      "### 4, done: False\n",
      "player 0, action 0, reward: 0.0\n",
      "4 / 10\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      "### 5, done: False\n",
      "player 0, action 0, reward: 0.0\n",
      "5 / 10\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      "### 6, done: False\n",
      "player 0, action 0, reward: 0.0\n",
      "6 / 10\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      "### 7, done: False\n",
      "player 0, action 1, reward: 0.0\n",
      "0 / 10\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      "### 8, done: False\n",
      "player 0, action 0, reward: 0.0\n",
      "1 / 10\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      "### 9, done: False\n",
      "player 0, action 0, reward: 0.0\n",
      "2 / 10\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      "### 10, done: False\n",
      "player 0, action 0, reward: 0.0\n",
      "3 / 10\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      "### 11, done: True\n",
      "player 0, action 0, reward: 0.0\n",
      "4 / 10\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "### env: OX, max episodes: -1, max steps: 10, timeout:  -1.00s\n",
      "17:53:36   0.00s       1ep       5tr  -0.00s(remain), -1.000 -1.000 -1.000 reward, 8.0 step, 0.00s/ep, 0.0000s/tr,        0 mem|Q 3.000|td_error -0.250\n",
      "### 0\n",
      "----------\n",
      "| 0| 1| 2|\n",
      "----------\n",
      "| 3| 4| 5|\n",
      "----------\n",
      "| 6| 7| 8|\n",
      "----------\n",
      "next player: 0\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      " 2: 0.00000\n",
      " 3: 0.00000\n",
      " 4: 0.00000\n",
      " 5: 0.00000\n",
      " 6: 0.00000\n",
      " 7: 0.00000\n",
      " 8: 0.00000\n",
      "### 1, done: False\n",
      "player 0, action 8, reward: 0.0\n",
      "----------\n",
      "| 0| 1| 2|\n",
      "----------\n",
      "| 3| 4| 5|\n",
      "----------\n",
      "| 6| 7| o|\n",
      "----------\n",
      "next player: 1\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      " 2: 0.00000\n",
      " 3: 0.00000\n",
      " 4: 0.00000\n",
      " 5: 0.00000\n",
      " 6: 0.00000\n",
      " 7: 0.00000\n",
      " 8: 0.00000\n",
      "### 2, done: False\n",
      "player 0, action 1, reward: 0.0\n",
      "----------\n",
      "| 0| x| 2|\n",
      "----------\n",
      "| 3| 4| 5|\n",
      "----------\n",
      "| 6| 7| o|\n",
      "----------\n",
      "next player: 0\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1: 0.00000\n",
      " 2: 0.00000\n",
      " 3: 0.00000\n",
      " 4: 0.00000\n",
      " 5: 0.00000\n",
      " 6: 0.00000\n",
      " 7: 0.00000\n",
      " 8: 0.00000\n",
      "### 3, done: False\n",
      "player 0, action 2, reward: 0.0\n",
      "----------\n",
      "| 0| x| o|\n",
      "----------\n",
      "| 3| 4| 5|\n",
      "----------\n",
      "| 6| 7| o|\n",
      "----------\n",
      "next player: 1\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1:    -inf\n",
      " 2: 0.00000\n",
      " 3: 0.00000\n",
      " 4: 0.00000\n",
      " 5: 0.00000\n",
      " 6: 0.00000\n",
      " 7: 0.00000\n",
      " 8:    -inf\n",
      "### 4, done: False\n",
      "player 0, action 4, reward: 0.0\n",
      "----------\n",
      "| 0| x| o|\n",
      "----------\n",
      "| 3| x| 5|\n",
      "----------\n",
      "| 6| 7| o|\n",
      "----------\n",
      "next player: 0\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1:    -inf\n",
      " 2: 0.00000\n",
      " 3: 0.00000\n",
      " 4: 0.00000\n",
      " 5: 0.00000\n",
      " 6: 0.00000\n",
      " 7: 0.00000\n",
      " 8:    -inf\n",
      "### 5, done: False\n",
      "player 0, action 7, reward: 0.0\n",
      "----------\n",
      "| 0| x| o|\n",
      "----------\n",
      "| 3| x| 5|\n",
      "----------\n",
      "| 6| o| o|\n",
      "----------\n",
      "next player: 1\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1:    -inf\n",
      " 2:    -inf\n",
      " 3: 0.00000\n",
      " 4:    -inf\n",
      " 5: 0.00000\n",
      " 6: 0.00000\n",
      " 7: 0.00000\n",
      " 8:    -inf\n",
      "### 6, done: False\n",
      "player 0, action 0, reward: 0.0\n",
      "----------\n",
      "| x| x| o|\n",
      "----------\n",
      "| 3| x| 5|\n",
      "----------\n",
      "| 6| o| o|\n",
      "----------\n",
      "next player: 0\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      "*0: 0.00000\n",
      " 1:    -inf\n",
      " 2:    -inf\n",
      " 3: 0.00000\n",
      " 4:    -inf\n",
      " 5: 0.00000\n",
      " 6: 0.00000\n",
      " 7: 0.00000\n",
      " 8:    -inf\n",
      "### 7, done: False\n",
      "player 0, action 3, reward: 0.0\n",
      "----------\n",
      "| x| x| o|\n",
      "----------\n",
      "| o| x| 5|\n",
      "----------\n",
      "| 6| o| o|\n",
      "----------\n",
      "next player: 1\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      " 0:    -inf\n",
      " 1:    -inf\n",
      " 2:    -inf\n",
      "*3: 0.00000\n",
      " 4:    -inf\n",
      " 5: 0.00000\n",
      " 6: 0.00000\n",
      " 7:    -inf\n",
      " 8:    -inf\n",
      "### 8, done: False\n",
      "player 0, action 5, reward: 0.0\n",
      "----------\n",
      "| x| x| o|\n",
      "----------\n",
      "| o| x| x|\n",
      "----------\n",
      "| 6| o| o|\n",
      "----------\n",
      "next player: 0\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      " 0:    -inf\n",
      " 1:    -inf\n",
      " 2:    -inf\n",
      "*3: 0.00000\n",
      " 4:    -inf\n",
      " 5: 0.00000\n",
      " 6: 0.00000\n",
      " 7:    -inf\n",
      " 8:    -inf\n",
      "### 9, done: True\n",
      "player 0, action 6, reward: 1.0\n",
      "----------\n",
      "| x| x| o|\n",
      "----------\n",
      "| o| x| x|\n",
      "----------\n",
      "| o| o| o|\n",
      "----------\n",
      "next player: 1\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n"
     ]
    }
   ],
   "source": [
    "from srl.test import TestRL\n",
    "\n",
    "tester = TestRL()\n",
    "tester.play_sequence(MyConfig())\n",
    "#tester.play_mp(MyConfig())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### env: Grid, max episodes: 10000, max steps: -1, timeout:  -1.00s\n",
      "17:53:41   5.00s    6202ep   45701tr   2.41s(remain), -2.000 0.673 0.840 reward, 7.4 step, 0.00s/ep, 0.0000s/tr,        0 mem|Q 10.999|td_error 0.014\n",
      "17:53:44   5.00s   10000ep   74404tr   0.00s(remain), -2.440 0.688 0.840 reward, 7.6 step, 0.00s/ep, 0.0000s/tr,        0 mem|Q 11.000|td_error 0.012\n"
     ]
    }
   ],
   "source": [
    "import srl\n",
    "from srl.runner import sequence\n",
    "from srl.runner.callbacks import PrintProgress\n",
    "\n",
    "config = sequence.Config(\n",
    "    env_config=srl.envs.Config(\"Grid\"),\n",
    "    rl_config=MyConfig(),\n",
    ")\n",
    "\n",
    "# --- train\n",
    "config.set_play_config(max_episodes=10000, training=True, callbacks=[PrintProgress()])\n",
    "parameter, remote_memory = sequence.train(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100エピソードの平均結果 0.784\n"
     ]
    }
   ],
   "source": [
    "config.set_play_config(max_episodes=100)\n",
    "rewards, _, _ = sequence.play(config, parameter)\n",
    "print(\"100エピソードの平均結果\", np.mean(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 0\n",
      "......\n",
      ".   G.\n",
      ". . X.\n",
      ".P   .\n",
      "......\n",
      "\n",
      " ←: 0.28983\n",
      " ↓: 0.27218\n",
      " →: 0.23325\n",
      "*↑: 0.36876\n",
      "### 1, done: False\n",
      "player 0, action 3, reward: -0.04\n",
      "......\n",
      ".   G.\n",
      ".P. X.\n",
      ".    .\n",
      "......\n",
      "\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      " ←: 0.28983\n",
      " ↓: 0.27218\n",
      " →: 0.23325\n",
      "*↑: 0.36876\n",
      "### 2, done: False\n",
      "player 0, action 3, reward: -0.04\n",
      "......\n",
      ".P  G.\n",
      ". . X.\n",
      ".    .\n",
      "......\n",
      "\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      " ←: 0.38713\n",
      " ↓: 0.31794\n",
      " →: 0.40086\n",
      "*↑: 0.48485\n",
      "### 3, done: False\n",
      "player 0, action 2, reward: -0.04\n",
      "......\n",
      ". P G.\n",
      ". . X.\n",
      ".    .\n",
      "......\n",
      "\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      " ←: 0.48868\n",
      " ↓: 0.43742\n",
      "*→: 0.60823\n",
      " ↑: 0.50167\n",
      "### 4, done: False\n",
      "player 0, action 2, reward: -0.04\n",
      "......\n",
      ".  PG.\n",
      ". . X.\n",
      ".    .\n",
      "......\n",
      "\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      " ←: 0.51996\n",
      " ↓: 0.62921\n",
      "*→: 0.78265\n",
      " ↑: 0.64996\n",
      "### 5, done: False\n",
      "player 0, action 2, reward: -0.04\n",
      "......\n",
      ".  PG.\n",
      ". . X.\n",
      ".    .\n",
      "......\n",
      "\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n",
      " ←: 0.68323\n",
      " ↓: 0.48729\n",
      "*→: 0.96802\n",
      " ↑: 0.81244\n",
      "### 6, done: True\n",
      "player 0, action 2, reward: 1.0\n",
      "......\n",
      ".   P.\n",
      ". . X.\n",
      ".    .\n",
      "......\n",
      "\n",
      "env_info  : {}\n",
      "work_info 0: None\n",
      "train_info: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from srl.runner.callbacks import Rendering\n",
    "\n",
    "config.set_play_config(max_episodes=1, callbacks=[Rendering(step_stop=False)])\n",
    "rewards, _, _ = sequence.play(config, parameter)\n",
    "rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea4fe4d9f7a0ac0973d73e7eb814bdfb578c3b05679fd701fb5853be0632daec"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('biz310': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
