@startuml

abstract gym.Env {
    action_space : gym.spaces.Space
    observation_space : gym.spaces.Space
    reset()
    step(action)
    render()
}

enum RLActionType {
    DISCRETE
    CONTINUOUS
}

enum RLObservationType {
    DISCRETE
    CONTINUOUS
}

enum EnvObservationType {
    DISCRETE
    CONTINUOUS
    GRAY_2ch
    GRAY_3ch
    COLOR
    SHAPE2
    SHAPE3
}

abstract EnvBase {
    action_space : gym.spaces.Space
    observation_space : gym.spaces.Space
    reset()
    step(action)
    render()
    - observation_type : EnvObservationType
    - max_episode_steps : int
    - fetch_valid_actions()
    - backup()
    - restore()
    - action_to_str()
} 

class EnvForRL

abstract RLRemoteMemory {
    length()
    backup()
    restore()
}


abstract RLConfig {
    {static} getName() : str
    action_type : RLActionType
    observation_type : RLObservationType
    assert_params()
    set_config_by_env(EnvForRL)
}

abstract RLParameter {
    restore()
    backup()
}

abstract RLTrainer {
    config : RLConfig
    memory : RLRemoteMemory
    board : ParameterBoard
    get_train_count() : int
    train()
}

abstract RLWorker {
    config : RLConfig
    memory : RLRemoteMemory
    board : ParameterBoard
    worker_id : int
    on_reset(state, valid_actions)
    policy(state, valid_actions)
    on_step(state, action, next_state, reward, done, valid_actions, next_valid_actions)
    render(state, valid_actions, action_to_str)
}


EnvBase <|-- gym.Env 
EnvForRL -- EnvBase

RLConfig -- RLWorker
RLConfig -- RLTrainer
RLConfig -- RLParameter
RLParameter -- RLWorker
RLParameter -- RLTrainer
RLRemoteMemory -- RLWorker
RLRemoteMemory -- RLTrainer

RLWorker .. EnvForRL
RLTrainer .. EnvForRL

@enduml